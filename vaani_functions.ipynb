{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e794e1dc-039e-4587-95bb-06fb48aa2473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully\n",
      "âœ… Master Registry Link configured\n",
      "ðŸ“ Link: https://docs.google.com/spreadsheets/d/e/2PACX-1vQdOVYDNLuMG...\n",
      "\n",
      "ðŸ”„ Testing registry load...\n",
      "âœ… Registry loaded successfully: 8 items found\n",
      "\n",
      "ðŸ“‹ Available items in registry:\n",
      "  1. default_model\n",
      "  2. openai_api_key\n",
      "  3. anthropic_api_key\n",
      "  4. master_registry_link\n",
      "  5. usage_data\n",
      "  6. dropoff_analysis\n",
      "  7. user_research\n",
      "  8. excel_db_1000_items\n",
      "\n",
      "ðŸ§ª Testing get_item() function:\n",
      "\n",
      "1. Testing with 'default_model':\n",
      "âœ… Registry loaded successfully: 8 items found\n",
      "âœ… Retrieved 'default_model'\n",
      "   Result: {'model': 'claude-sonnet-4-20250514', 'temperature': 0.3, 'max_tokens': 1000, 'provider': 'anthropic'}\n",
      "\n",
      "2. Testing with non-existent item:\n",
      "âœ… Registry loaded successfully: 8 items found\n",
      "âŒ Item 'this_does_not_exist' not found in registry\n",
      "ðŸ’¡ Available items: default_model, openai_api_key, anthropic_api_key, master_registry_link, usage_data, dropoff_analysis, user_research, excel_db_1000_items\n",
      "\n",
      "3. Testing with Sarvam API (might not have data yet):\n",
      "âœ… Registry loaded successfully: 8 items found\n",
      "âŒ Item 'sarvam_api' not found in registry\n",
      "ðŸ’¡ Available items: default_model, openai_api_key, anthropic_api_key, master_registry_link, usage_data, dropoff_analysis, user_research, excel_db_1000_items\n",
      "\n",
      "ðŸ“¦ Loading all items from registry:\n",
      "âœ… Registry loaded successfully: 8 items found\n",
      "âœ… Successfully loaded 8 items\n",
      "\n",
      "âœ… Available configurations:\n",
      "  â€¢ default_model: ['model', 'temperature', 'max_tokens', 'provider']\n",
      "  â€¢ openai_api_key: ['key', 'description']\n",
      "  â€¢ anthropic_api_key: ['key', 'description']\n",
      "  â€¢ master_registry_link: ['link', 'description']\n",
      "  â€¢ usage_data: ['link', 'local_path', 'description']\n",
      "  â€¢ dropoff_analysis: ['link', 'local_path', 'description']\n",
      "  â€¢ user_research: ['link', 'local_path', 'description']\n",
      "  â€¢ excel_db_1000_items: ['link', 'local_path', 'description']\n",
      "\n",
      "ðŸ§ª Testing helper functions:\n",
      "\n",
      "1. API Keys:\n",
      "âœ… Registry loaded successfully: 8 items found\n",
      "   OpenAI Key: sk-YOUR_KEY_HERE... (length: 16)\n",
      "\n",
      "2. Model Config:\n",
      "âœ… Registry loaded successfully: 8 items found\n",
      "   {'model': 'claude-sonnet-4-20250514', 'temperature': 0.3, 'max_tokens': 1000, 'provider': 'anthropic'}\n",
      "\n",
      "3. Data File Links:\n",
      "âœ… Registry loaded successfully: 8 items found\n",
      "   Usage Data: https://docs.google.com/spreadsheets/d/SHEET_ID...\n",
      "ðŸ¥ REGISTRY HEALTH CHECK\n",
      "============================================================\n",
      "âœ… Registry loaded successfully: 8 items found\n",
      "âœ… Successfully loaded 8 items\n",
      "\n",
      "ðŸ“‹ API Keys:\n",
      "   âš ï¸ Placeholder openai_api_key\n",
      "   âœ… anthropic_api_key\n",
      "\n",
      "ðŸ“‹ Models:\n",
      "   âœ… default_model\n",
      "\n",
      "ðŸ“‹ Data Files:\n",
      "   âœ… usage_data\n",
      "   âœ… dropoff_analysis\n",
      "   âœ… user_research\n",
      "   âœ… excel_db_1000_items\n",
      "\n",
      "ðŸ“‹ Config:\n",
      "   âœ… master_registry_link\n",
      "\n",
      "============================================================\n",
      "âœ… Total items loaded: 8\n",
      "\n",
      "ðŸ“š HOW TO USE THIS CONFIG IN OTHER NOTEBOOKS\n",
      "=====================================================\n",
      "\n",
      "1ï¸âƒ£ LOAD CONFIG IN ANY NOTEBOOK:\n",
      "   %run vyapar_config.ipynb\n",
      "\n",
      "2ï¸âƒ£ GET AN ITEM:\n",
      "   model = get_item('default_model')\n",
      "   \n",
      "3ï¸âƒ£ GET API KEY:\n",
      "   api_key = get_api_key('openai_api_key')\n",
      "   \n",
      "4ï¸âƒ£ GET DATA FILE LINK:\n",
      "   link = get_data_file_link('usage_data')\n",
      "   df = pd.read_excel(link)\n",
      "   \n",
      "5ï¸âƒ£ GET ALL ITEMS:\n",
      "   all_configs = get_all_items()\n",
      "   \n",
      "6ï¸âƒ£ CHECK HEALTH:\n",
      "   check_registry_health()\n",
      "\n",
      "=====================================================\n",
      "âœ… Config loaded and ready to use!\n",
      "\n",
      "âœ… Registry loaded successfully: 8 items found\n",
      "Default model: claude-sonnet-4-20250514\n",
      "Provider: anthropic\n",
      "âœ… Claude is now the default!\n",
      "============================================================\n",
      "ðŸ“ VAANI PROMPTS LIBRARY\n",
      "============================================================\n",
      "This file contains all system prompts for AI agents\n",
      "Organized by transaction type and use case\n",
      "============================================================\n",
      "âœ… Intent Detection Prompt loaded\n",
      "âœ… Expense Extraction Prompt loaded\n",
      "âœ… Sale Extraction Prompt loaded\n",
      "âœ… Purchase Extraction Prompt loaded\n",
      "âœ… Payment In Extraction Prompt loaded\n",
      "âœ… Payment Out Extraction Prompt loaded\n",
      "âœ… Multi-Item Extraction Prompt loaded\n",
      "\n",
      "ðŸ§ª Testing Prompt Helper:\n",
      "ðŸ“ AVAILABLE PROMPTS:\n",
      "============================================================\n",
      "  intent          â†’ Intent Detection - Classifies transaction type\n",
      "  expense         â†’ Expense Extraction - Extracts amount, item, category\n",
      "  sale            â†’ Sale Extraction - Extracts customer, items, amount\n",
      "  purchase        â†’ Purchase Extraction - Extracts supplier, inventory, amount\n",
      "  payment_in      â†’ Payment In - Extracts payment received details\n",
      "  payment_out     â†’ Payment Out - Extracts payment made details\n",
      "  multi_item      â†’ Multi-Item - Handles multiple items in one input\n",
      "============================================================\n",
      "\n",
      "âœ… Example usage:\n",
      "  prompt = get_prompt('expense', 'chai 60 rupees')\n",
      "\n",
      "ðŸ“š VAANI PROMPTS LIBRARY - SUMMARY\n",
      "=====================================================\n",
      "\n",
      "AVAILABLE PROMPTS:\n",
      "1. Intent Detection     â†’ Classify transaction type\n",
      "2. Expense Extraction   â†’ Extract expense details\n",
      "3. Sale Extraction      â†’ Extract sale details\n",
      "4. Purchase Extraction  â†’ Extract purchase details\n",
      "5. Payment In           â†’ Extract payment received\n",
      "6. Payment Out          â†’ Extract payment made\n",
      "7. Multi-Item           â†’ Handle multiple items\n",
      "\n",
      "HOW TO USE:\n",
      "-----------\n",
      "# Get a specific prompt\n",
      "prompt = get_prompt('expense', text_input='chai 60 rupees')\n",
      "\n",
      "# List all available prompts\n",
      "list_available_prompts()\n",
      "\n",
      "# Use in your agent functions\n",
      "# (See vaani_functions.ipynb for implementation)\n",
      "\n",
      "CHAINING LOGIC:\n",
      "---------------\n",
      "User Input â†’ Intent Detection â†’ Route to Transaction-Specific Prompt\n",
      "\n",
      "If transaction_type known:\n",
      "  Skip intent detection, go directly to transaction prompt\n",
      "\n",
      "=====================================================\n",
      "âœ… Prompts library ready!\n",
      "\n",
      "âœ… vaani_functions.ipynb loaded\n",
      "âœ… Config available via get_item(), get_api_key(), etc.\n",
      "âœ… Prompts library loaded\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 1: SETUP\n",
    "Description: Import required libraries, load configuration, and load prompts\n",
    "\"\"\"\n",
    "\n",
    "# Load configuration\n",
    "%run vyapar_config.ipynb\n",
    "\n",
    "# Load prompts library\n",
    "%run vaani_prompts.ipynb\n",
    "\n",
    "# Additional imports for AI functions\n",
    "import openai\n",
    "from anthropic import Anthropic\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "print(\"âœ… vaani_functions.ipynb loaded\")\n",
    "print(\"âœ… Config available via get_item(), get_api_key(), etc.\")\n",
    "print(\"âœ… Prompts library loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23fc7849-a154-42d8-856e-310b21e10135",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL 2: OPENAI CLIENT SETUP\n",
    "Description: Create OpenAI client using API key from registry\n",
    "\"\"\"\n",
    "\n",
    "def get_openai_client():\n",
    "    \"\"\"\n",
    "    Initialize and return OpenAI client.\n",
    "    Uses API key from registry.\n",
    "    \"\"\"\n",
    "    api_key = get_api_key('openai_api_key')\n",
    "    \n",
    "    if not api_key or 'YOUR' in api_key:\n",
    "        print(\"âš ï¸ OpenAI API key not configured properly\")\n",
    "        return None\n",
    "    \n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    "    print(\"âœ… OpenAI client initialized\")\n",
    "    return client\n",
    "\n",
    "# Test it\n",
    "# openai_client = get_openai_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3a22d4d-2e5d-4f0e-84f4-23c2398ab97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL 3: ANTHROPIC CLIENT SETUP\n",
    "Description: Create Anthropic client using API key from registry\n",
    "\"\"\"\n",
    "\n",
    "def get_anthropic_client():\n",
    "    \"\"\"\n",
    "    Initialize and return Anthropic client.\n",
    "    Uses API key from registry.\n",
    "    \"\"\"\n",
    "    api_key = get_api_key('anthropic_api_key')\n",
    "    \n",
    "    if not api_key or 'YOUR' in api_key:\n",
    "        print(\"âš ï¸ Anthropic API key not configured properly\")\n",
    "        return None\n",
    "    \n",
    "    client = Anthropic(api_key=api_key)\n",
    "    print(\"âœ… Anthropic client initialized\")\n",
    "    return client\n",
    "\n",
    "# Test it\n",
    "# anthropic_client = get_anthropic_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af4cbbec-41b5-490f-9add-b05ce855a7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing Basic Extractor Agent:\n",
      "âœ… Registry loaded successfully: 8 items found\n",
      "âœ… Registry loaded successfully: 8 items found\n",
      "âš ï¸ OpenAI API key not configured properly\n",
      "{'error': 'OpenAI client not available'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 4: AI AGENT 1 - BASIC EXTRACTOR\n",
    "Description: Extracts amount, item name, and category from voice input\n",
    "Use case: Quick, simple extractions\n",
    "\"\"\"\n",
    "\n",
    "def agent_basic_extractor(voice_input: str, model: str = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Basic extraction agent: Extracts amount, item, category from voice input.\n",
    "    \n",
    "    Args:\n",
    "        voice_input: The voice transcription (e.g., \"chai samosa 140 rupees\")\n",
    "        model: Model to use (defaults to registry default_model)\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            'amount': 140,\n",
    "            'item': 'Chai Samosa',\n",
    "            'category': 'Food',\n",
    "            'raw_response': '...',\n",
    "            'model_used': 'gpt-4o-mini',\n",
    "            'time_taken': 1.2\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Get model config\n",
    "    if model is None:\n",
    "        model_config = get_model_config()\n",
    "        model = model_config['model']\n",
    "    else:\n",
    "        model_config = get_model_config(model)\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"You are an expense tracking assistant. Extract the following from the voice input:\n",
    "\n",
    "Voice Input: \"{voice_input}\"\n",
    "\n",
    "Extract and return ONLY a JSON object with these fields:\n",
    "- amount: The amount spent (number only, no currency)\n",
    "- item: The item name\n",
    "- category: Best category for this expense (Food, Transport, Utilities, etc.)\n",
    "\n",
    "Return ONLY valid JSON, nothing else.\"\"\"\n",
    "\n",
    "    # Call LLM\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        if 'gpt' in model:\n",
    "            # OpenAI call\n",
    "            client = get_openai_client()\n",
    "            if not client:\n",
    "                return {'error': 'OpenAI client not available'}\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=model_config.get('temperature', 0.3),\n",
    "                max_tokens=model_config.get('max_tokens', 500)\n",
    "            )\n",
    "            raw_response = response.choices[0].message.content\n",
    "        \n",
    "        elif 'claude' in model:\n",
    "            # Anthropic call\n",
    "            client = get_anthropic_client()\n",
    "            if not client:\n",
    "                return {'error': 'Anthropic client not available'}\n",
    "            \n",
    "            response = client.messages.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=model_config.get('temperature', 0.3),\n",
    "                max_tokens=model_config.get('max_tokens', 500)\n",
    "            )\n",
    "            raw_response = response.content[0].text\n",
    "        \n",
    "        else:\n",
    "            return {'error': f'Unknown model: {model}'}\n",
    "        \n",
    "        time_taken = time.time() - start_time\n",
    "        \n",
    "        # Parse JSON response\n",
    "        import json\n",
    "        extracted = json.loads(raw_response)\n",
    "        \n",
    "        # Add metadata\n",
    "        extracted['raw_response'] = raw_response\n",
    "        extracted['model_used'] = model\n",
    "        extracted['time_taken'] = round(time_taken, 2)\n",
    "        \n",
    "        return extracted\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'error': str(e),\n",
    "            'model_used': model,\n",
    "            'time_taken': round(time.time() - start_time, 2)\n",
    "        }\n",
    "\n",
    "# Test it\n",
    "print(\"ðŸ§ª Testing Basic Extractor Agent:\")\n",
    "test_result = agent_basic_extractor(\"chai samosa 140 rupees\")\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0afc04c7-e5ea-4889-b468-f1b2510cc086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing Smart Categorizer Agent:\n",
      "âœ… Registry loaded successfully: 8 items found\n",
      "âœ… Registry loaded successfully: 8 items found\n",
      "âš ï¸ OpenAI API key not configured properly\n",
      "{'error': 'OpenAI client not available'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 5: AI AGENT 2 - SMART CATEGORIZER\n",
    "Description: Uses context from Excel DB for more accurate categorization\n",
    "Use case: When you want high accuracy categorization\n",
    "\"\"\"\n",
    "\n",
    "def agent_smart_categorizer(voice_input: str, excel_db_context: str = None, model: str = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Smart categorization agent: Better category matching using Excel DB context.\n",
    "    \n",
    "    Args:\n",
    "        voice_input: The voice transcription\n",
    "        excel_db_context: Sample categories from Excel DB (optional)\n",
    "        model: Model to use (defaults to registry default_model)\n",
    "    \n",
    "    Returns:\n",
    "        Same as agent_basic_extractor but with improved categorization\n",
    "    \"\"\"\n",
    "    # Get model config\n",
    "    if model is None:\n",
    "        model_config = get_model_config()\n",
    "        model = model_config['model']\n",
    "    else:\n",
    "        model_config = get_model_config(model)\n",
    "    \n",
    "    # Add context to prompt if provided\n",
    "    context_section = \"\"\n",
    "    if excel_db_context:\n",
    "        context_section = f\"\\n\\nCommon categories used by similar businesses:\\n{excel_db_context}\\n\"\n",
    "    \n",
    "    # Create enhanced prompt\n",
    "    prompt = f\"\"\"You are an expense tracking assistant for Indian MSMEs. Extract information from this voice input.\n",
    "\n",
    "Voice Input: \"{voice_input}\"\n",
    "{context_section}\n",
    "Extract and return ONLY a JSON object:\n",
    "- amount: Amount spent (number only)\n",
    "- item: Item name\n",
    "- category: Most appropriate category (consider common Indian business expenses)\n",
    "\n",
    "Return ONLY valid JSON.\"\"\"\n",
    "\n",
    "    # Call LLM (same logic as basic extractor)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        if 'gpt' in model:\n",
    "            client = get_openai_client()\n",
    "            if not client:\n",
    "                return {'error': 'OpenAI client not available'}\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=model_config.get('temperature', 0.3),\n",
    "                max_tokens=model_config.get('max_tokens', 500)\n",
    "            )\n",
    "            raw_response = response.choices[0].message.content\n",
    "        \n",
    "        elif 'claude' in model:\n",
    "            client = get_anthropic_client()\n",
    "            if not client:\n",
    "                return {'error': 'Anthropic client not available'}\n",
    "            \n",
    "            response = client.messages.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=model_config.get('temperature', 0.3),\n",
    "                max_tokens=model_config.get('max_tokens', 500)\n",
    "            )\n",
    "            raw_response = response.content[0].text\n",
    "        \n",
    "        else:\n",
    "            return {'error': f'Unknown model: {model}'}\n",
    "        \n",
    "        time_taken = time.time() - start_time\n",
    "        \n",
    "        # Parse JSON\n",
    "        import json\n",
    "        extracted = json.loads(raw_response)\n",
    "        \n",
    "        # Add metadata\n",
    "        extracted['raw_response'] = raw_response\n",
    "        extracted['model_used'] = model\n",
    "        extracted['time_taken'] = round(time_taken, 2)\n",
    "        extracted['used_context'] = excel_db_context is not None\n",
    "        \n",
    "        return extracted\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'error': str(e),\n",
    "            'model_used': model,\n",
    "            'time_taken': round(time.time() - start_time, 2)\n",
    "        }\n",
    "\n",
    "# Test it\n",
    "print(\"ðŸ§ª Testing Smart Categorizer Agent:\")\n",
    "test_result = agent_smart_categorizer(\"petrol 500 rupees\")\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3ae9b22-5b3a-4716-a3d3-9866b2422dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing Multi-Item Handler Agent:\n",
      "âœ… Registry loaded successfully: 8 items found\n",
      "âœ… Registry loaded successfully: 8 items found\n",
      "âš ï¸ OpenAI API key not configured properly\n",
      "{'error': 'OpenAI client not available'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 6: AI AGENT 3 - MULTI-ITEM HANDLER\n",
    "Description: Handles voice inputs with multiple items\n",
    "Use case: \"chai 60 rupees, samosa 80 rupees\"\n",
    "\"\"\"\n",
    "\n",
    "def agent_multi_item_handler(voice_input: str, model: str = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Multi-item handler: Extracts multiple items from single voice input.\n",
    "    \n",
    "    Args:\n",
    "        voice_input: Voice input with multiple items\n",
    "        model: Model to use\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            'items': [\n",
    "                {'amount': 60, 'item': 'Chai', 'category': 'Food'},\n",
    "                {'amount': 80, 'item': 'Samosa', 'category': 'Food'}\n",
    "            ],\n",
    "            'total_amount': 140,\n",
    "            'item_count': 2,\n",
    "            'model_used': 'gpt-4o-mini',\n",
    "            'time_taken': 1.5\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Get model config\n",
    "    if model is None:\n",
    "        model_config = get_model_config()\n",
    "        model = model_config['model']\n",
    "    else:\n",
    "        model_config = get_model_config(model)\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"You are an expense tracking assistant. The voice input may contain multiple items.\n",
    "\n",
    "Voice Input: \"{voice_input}\"\n",
    "\n",
    "Extract ALL items mentioned. Return ONLY a JSON object:\n",
    "{{\n",
    "    \"items\": [\n",
    "        {{\"amount\": number, \"item\": \"name\", \"category\": \"category\"}},\n",
    "        ...\n",
    "    ],\n",
    "    \"total_amount\": sum of all amounts\n",
    "}}\n",
    "\n",
    "Return ONLY valid JSON.\"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        if 'gpt' in model:\n",
    "            client = get_openai_client()\n",
    "            if not client:\n",
    "                return {'error': 'OpenAI client not available'}\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=model_config.get('temperature', 0.3),\n",
    "                max_tokens=model_config.get('max_tokens', 800)\n",
    "            )\n",
    "            raw_response = response.choices[0].message.content\n",
    "        \n",
    "        elif 'claude' in model:\n",
    "            client = get_anthropic_client()\n",
    "            if not client:\n",
    "                return {'error': 'Anthropic client not available'}\n",
    "            \n",
    "            response = client.messages.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=model_config.get('temperature', 0.3),\n",
    "                max_tokens=model_config.get('max_tokens', 800)\n",
    "            )\n",
    "            raw_response = response.content[0].text\n",
    "        \n",
    "        else:\n",
    "            return {'error': f'Unknown model: {model}'}\n",
    "        \n",
    "        time_taken = time.time() - start_time\n",
    "        \n",
    "        # Parse JSON\n",
    "        import json\n",
    "        extracted = json.loads(raw_response)\n",
    "        \n",
    "        # Add metadata\n",
    "        extracted['item_count'] = len(extracted.get('items', []))\n",
    "        extracted['raw_response'] = raw_response\n",
    "        extracted['model_used'] = model\n",
    "        extracted['time_taken'] = round(time_taken, 2)\n",
    "        \n",
    "        return extracted\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'error': str(e),\n",
    "            'model_used': model,\n",
    "            'time_taken': round(time.time() - start_time, 2)\n",
    "        }\n",
    "\n",
    "# Test it\n",
    "print(\"ðŸ§ª Testing Multi-Item Handler Agent:\")\n",
    "test_result = agent_multi_item_handler(\"chai 60 rupees, samosa 80 rupees\")\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20683d83-caf4-4bed-99af-f03a1f9fc270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª Testing Agent Router:\n",
      "\n",
      "1. Single item:\n",
      "ðŸŽ¯ Routing to: Basic Extractor\n",
      "âœ… Registry loaded successfully: 8 items found\n",
      "âœ… Registry loaded successfully: 8 items found\n",
      "âš ï¸ OpenAI API key not configured properly\n",
      "Result: {'error': 'OpenAI client not available', 'agent_used': 'basic_extractor'}\n",
      "\n",
      "2. Multiple items:\n",
      "ðŸŽ¯ Routing to: Multi-Item Handler\n",
      "âœ… Registry loaded successfully: 8 items found\n",
      "âœ… Registry loaded successfully: 8 items found\n",
      "âš ï¸ OpenAI API key not configured properly\n",
      "Result: {'error': 'OpenAI client not available', 'agent_used': 'multi_item_handler'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 7: AGENT ROUTER\n",
    "Description: Intelligently routes to the right agent based on input complexity\n",
    "\"\"\"\n",
    "\n",
    "def route_to_agent(voice_input: str, model: str = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Smart router: Analyzes input and calls the appropriate agent.\n",
    "    \n",
    "    Logic:\n",
    "    - If multiple items detected â†’ Multi-Item Handler\n",
    "    - If simple input â†’ Basic Extractor\n",
    "    - If needs context â†’ Smart Categorizer\n",
    "    \n",
    "    Args:\n",
    "        voice_input: The voice input\n",
    "        model: Model to use (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Result from the chosen agent + routing metadata\n",
    "    \"\"\"\n",
    "    # Simple heuristics for routing\n",
    "    voice_lower = voice_input.lower()\n",
    "    \n",
    "    # Check for multiple items (commas, \"and\", multiple numbers)\n",
    "    has_comma = ',' in voice_input\n",
    "    has_and = ' and ' in voice_lower or ' aur ' in voice_lower\n",
    "    number_count = sum(c.isdigit() for c in voice_input)\n",
    "    \n",
    "    if has_comma or has_and or number_count > 6:\n",
    "        # Route to multi-item handler\n",
    "        print(\"ðŸŽ¯ Routing to: Multi-Item Handler\")\n",
    "        result = agent_multi_item_handler(voice_input, model)\n",
    "        result['agent_used'] = 'multi_item_handler'\n",
    "    \n",
    "    else:\n",
    "        # Route to basic extractor\n",
    "        print(\"ðŸŽ¯ Routing to: Basic Extractor\")\n",
    "        result = agent_basic_extractor(voice_input, model)\n",
    "        result['agent_used'] = 'basic_extractor'\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test it\n",
    "print(\"\\nðŸ§ª Testing Agent Router:\")\n",
    "print(\"\\n1. Single item:\")\n",
    "result1 = route_to_agent(\"chai 60 rupees\")\n",
    "print(f\"Result: {result1}\\n\")\n",
    "\n",
    "print(\"2. Multiple items:\")\n",
    "result2 = route_to_agent(\"chai 60 rupees, samosa 80 rupees\")\n",
    "print(f\"Result: {result2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6efa60d3-d633-43d9-8824-91617fc559fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing Utility Functions:\n",
      "Format 'â‚¹1,500': 1500.0\n",
      "Validate good result: (True, [])\n",
      "Validate bad result: (False, ['API Error: Failed'])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 8: UTILITY FUNCTIONS\n",
    "Description: Helper functions for formatting, validation, etc.\n",
    "\"\"\"\n",
    "\n",
    "def format_amount(amount: any) -> float:\n",
    "    \"\"\"Convert amount to float, handle different formats\"\"\"\n",
    "    try:\n",
    "        if isinstance(amount, str):\n",
    "            # Remove currency symbols, commas\n",
    "            amount = amount.replace('â‚¹', '').replace(',', '').strip()\n",
    "        return float(amount)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def validate_extraction(result: Dict) -> Tuple[bool, List[str]]:\n",
    "    \"\"\"\n",
    "    Validate extraction result.\n",
    "    Returns: (is_valid, list_of_errors)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    if 'error' in result:\n",
    "        errors.append(f\"API Error: {result['error']}\")\n",
    "        return False, errors\n",
    "    \n",
    "    # Check required fields\n",
    "    if 'amount' not in result:\n",
    "        errors.append(\"Missing amount\")\n",
    "    elif not isinstance(result['amount'], (int, float)):\n",
    "        errors.append(\"Amount is not a number\")\n",
    "    \n",
    "    if 'item' not in result:\n",
    "        errors.append(\"Missing item name\")\n",
    "    elif not result['item']:\n",
    "        errors.append(\"Item name is empty\")\n",
    "    \n",
    "    return len(errors) == 0, errors\n",
    "\n",
    "# Test utilities\n",
    "print(\"ðŸ§ª Testing Utility Functions:\")\n",
    "print(f\"Format 'â‚¹1,500': {format_amount('â‚¹1,500')}\")\n",
    "print(f\"Validate good result: {validate_extraction({'amount': 100, 'item': 'Chai'})}\")\n",
    "print(f\"Validate bad result: {validate_extraction({'error': 'Failed'})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "192b49e7-b398-4621-bd58-d60b94263400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“š VAANI FUNCTIONS AVAILABLE\n",
      "=====================================================\n",
      "\n",
      "ðŸ¤– AI AGENTS:\n",
      "1. agent_basic_extractor(voice_input, model=None)\n",
      "   - Simple, fast extraction\n",
      "   \n",
      "2. agent_smart_categorizer(voice_input, excel_db_context=None, model=None)\n",
      "   - Better categorization with context\n",
      "   \n",
      "3. agent_multi_item_handler(voice_input, model=None)\n",
      "   - Handles multiple items in one input\n",
      "   \n",
      "4. route_to_agent(voice_input, model=None)\n",
      "   - Smart router (RECOMMENDED)\n",
      "\n",
      "ðŸ› ï¸ UTILITIES:\n",
      "- format_amount(amount) - Clean amount formatting\n",
      "- validate_extraction(result) - Validate extraction results\n",
      "\n",
      "ðŸ”Œ CLIENTS:\n",
      "- get_openai_client() - Get OpenAI client\n",
      "- get_anthropic_client() - Get Anthropic client\n",
      "\n",
      "=====================================================\n",
      "âœ… All functions loaded and ready!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 9: SUMMARY\n",
    "Description: Shows all available functions in this notebook\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ“š VAANI FUNCTIONS AVAILABLE\n",
    "=====================================================\n",
    "\n",
    "ðŸ¤– AI AGENTS:\n",
    "1. agent_basic_extractor(voice_input, model=None)\n",
    "   - Simple, fast extraction\n",
    "   \n",
    "2. agent_smart_categorizer(voice_input, excel_db_context=None, model=None)\n",
    "   - Better categorization with context\n",
    "   \n",
    "3. agent_multi_item_handler(voice_input, model=None)\n",
    "   - Handles multiple items in one input\n",
    "   \n",
    "4. route_to_agent(voice_input, model=None)\n",
    "   - Smart router (RECOMMENDED)\n",
    "\n",
    "ðŸ› ï¸ UTILITIES:\n",
    "- format_amount(amount) - Clean amount formatting\n",
    "- validate_extraction(result) - Validate extraction results\n",
    "\n",
    "ðŸ”Œ CLIENTS:\n",
    "- get_openai_client() - Get OpenAI client\n",
    "- get_anthropic_client() - Get Anthropic client\n",
    "\n",
    "=====================================================\n",
    "âœ… All functions loaded and ready!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af08292e-1baa-4cad-9001-a9c4ee13bbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Category helper functions loaded\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 9A: CATEGORY HELPER FUNCTIONS\n",
    "Description: Load and format consolidated categories\n",
    "\"\"\"\n",
    "\n",
    "def load_categories_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load consolidated categories from registry.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with category data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        link = get_data_file_link('consolidated_categories')\n",
    "        if not link:\n",
    "            print(\"âš ï¸ Categories not found in registry\")\n",
    "            return None\n",
    "        \n",
    "        df = pd.read_excel(link)\n",
    "        print(f\"âœ… Loaded {len(df)} categories\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading categories: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def format_categories_for_prompt(df: pd.DataFrame = None, top_n: int = 50) -> str:\n",
    "    \"\"\"\n",
    "    Format categories as text for inclusion in prompts.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with categories (loads if None)\n",
    "        top_n: Number of top categories to include\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string for prompt\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        df = load_categories_data()\n",
    "        if df is None:\n",
    "            return \"\"\n",
    "    \n",
    "    categories_text = \"\"\n",
    "    for idx, row in df.head(top_n).iterrows():\n",
    "        rank = row.get('#', idx + 1)\n",
    "        category = row.get('Consolidated Category', 'Unknown')\n",
    "        notes = row.get('Notes', '')\n",
    "        \n",
    "        categories_text += f\"{rank}. {category}\"\n",
    "        if notes:\n",
    "            categories_text += f\" - {notes}\"\n",
    "        categories_text += \"\\n\"\n",
    "    \n",
    "    return categories_text.strip()\n",
    "\n",
    "\n",
    "def get_top_categories(df: pd.DataFrame = None, n: int = 10) -> list:\n",
    "    \"\"\"\n",
    "    Get list of top N category names.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with categories\n",
    "        n: Number to return\n",
    "    \n",
    "    Returns:\n",
    "        List of category names\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        df = load_categories_data()\n",
    "        if df is None:\n",
    "            return []\n",
    "    \n",
    "    return df.head(n)['Consolidated Category'].tolist()\n",
    "\n",
    "\n",
    "print(\"âœ… Category helper functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0dcec14b-d74c-4293-8fd0-439cda6b8012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… JSON extraction helper loaded\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 9B: JSON EXTRACTION HELPER\n",
    "Description: Safely extract JSON from LLM responses\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_json_from_response(response_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract JSON from LLM response, handling markdown code blocks and extra text.\n",
    "    \n",
    "    Args:\n",
    "        response_text: Raw response from LLM\n",
    "    \n",
    "    Returns:\n",
    "        Clean JSON string\n",
    "    \"\"\"\n",
    "    # Remove markdown code blocks\n",
    "    response_text = re.sub(r'```json\\s*', '', response_text)\n",
    "    response_text = re.sub(r'```\\s*', '', response_text)\n",
    "    \n",
    "    # Try to find JSON object in the text\n",
    "    json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "    \n",
    "    if json_match:\n",
    "        return json_match.group(0)\n",
    "    \n",
    "    return response_text.strip()\n",
    "\n",
    "print(\"âœ… JSON extraction helper loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00521bfd-d644-44a3-990f-c6cb3c7a759b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Timer utility loaded\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 9C: TIMER UTILITY\n",
    "Description: Clean timer wrapper for tracking cell execution time\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "\n",
    "class CellTimer:\n",
    "    \"\"\"Simple timer for tracking cell execution.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def elapsed(self) -> float:\n",
    "        \"\"\"Get elapsed time in seconds.\"\"\"\n",
    "        return time.time() - self.start_time\n",
    "    \n",
    "    def elapsed_ms(self) -> int:\n",
    "        \"\"\"Get elapsed time in milliseconds.\"\"\"\n",
    "        return int(self.elapsed() * 1000)\n",
    "    \n",
    "    def print_summary(self, label: str = \"CELL EXECUTION\"):\n",
    "        \"\"\"Print formatted timer summary.\"\"\"\n",
    "        elapsed = self.elapsed()\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"â±ï¸ {label} TIME: {elapsed*1000:.0f} ms ({elapsed:.2f}s)\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "print(\"âœ… Timer utility loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b89795b4-944d-4e59-ad47-aaccc9376eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Playground helper functions loaded\n",
      "\n",
      "ðŸ’¡ Available functions:\n",
      "   â€¢ view_prompt('expense')\n",
      "   â€¢ view_all_prompts()\n",
      "   â€¢ view_transaction_fields()\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 9D: PLAYGROUND HELPER FUNCTIONS\n",
    "Description: Utilities for viewing prompts and field requirements\n",
    "\"\"\"\n",
    "\n",
    "def view_prompt(prompt_type: str):\n",
    "    \"\"\"\n",
    "    View a system prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt_type: 'intent', 'expense', 'sale', 'purchase', 'payment_in', 'payment_out', 'multi_item'\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ðŸ“‹ PROMPT: {prompt_type.upper()}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    prompt = get_prompt(prompt_type, text_input=\"[USER_INPUT_HERE]\")\n",
    "    print(prompt)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "\n",
    "def view_all_prompts():\n",
    "    \"\"\"View all available prompts.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"ðŸ“š ALL AVAILABLE PROMPTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    prompts = list_available_prompts()\n",
    "    for idx, (key, desc) in enumerate(prompts.items(), 1):\n",
    "        print(f\"{idx}. {key}: {desc}\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Usage: view_prompt('expense')\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "def view_transaction_fields():\n",
    "    \"\"\"\n",
    "    View necessary and additional fields for each transaction type.\n",
    "    Based on PRD specifications.\n",
    "    \"\"\"\n",
    "    \n",
    "    fields_schema = {\n",
    "        'expense': {\n",
    "            'necessary': ['amount', 'item'],\n",
    "            'additional': ['category', 'date', 'payment_type', 'notes'],\n",
    "            'description': 'Recording business expenses'\n",
    "        },\n",
    "        'sale': {\n",
    "            'necessary': ['customer_name', 'amount', 'items'],\n",
    "            'additional': ['payment_type', 'date', 'notes', 'invoice_number'],\n",
    "            'description': 'Recording sales/invoices'\n",
    "        },\n",
    "        'purchase': {\n",
    "            'necessary': ['supplier_name', 'amount', 'items'],\n",
    "            'additional': ['payment_type', 'date', 'notes', 'inventory_update'],\n",
    "            'description': 'Recording purchases from suppliers'\n",
    "        },\n",
    "        'payment_in': {\n",
    "            'necessary': ['payer_name', 'amount'],\n",
    "            'additional': ['payment_type', 'date', 'notes', 'invoice_reference'],\n",
    "            'description': 'Money received from customers'\n",
    "        },\n",
    "        'payment_out': {\n",
    "            'necessary': ['payee_name', 'amount'],\n",
    "            'additional': ['payment_type', 'date', 'notes', 'invoice_reference'],\n",
    "            'description': 'Money paid to vendors/suppliers'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"ðŸ“Š TRANSACTION FIELD REQUIREMENTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for tx_type, fields in fields_schema.items():\n",
    "        print(f\"\\nðŸ”¹ {tx_type.upper()}\")\n",
    "        print(f\"   Description: {fields['description']}\")\n",
    "        print(f\"   âœ… Necessary: {', '.join(fields['necessary'])}\")\n",
    "        print(f\"   ðŸ“ Additional: {', '.join(fields['additional'])}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "\n",
    "print(\"âœ… Playground helper functions loaded\")\n",
    "print(\"\\nðŸ’¡ Available functions:\")\n",
    "print(\"   â€¢ view_prompt('expense')\")\n",
    "print(\"   â€¢ view_all_prompts()\")\n",
    "print(\"   â€¢ view_transaction_fields()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1fd346-0224-47b6-a593-2f72188acd7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24c0f8d8-83c6-45ba-8901-8d2c2587b2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Intent detection agent loaded\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 10: INTENT DETECTION AGENT (UPDATED)\n",
    "Description: Identifies if input is relevant and classifies transaction type\n",
    "\"\"\"\n",
    "\n",
    "def agent_intent_detector(text_input: str, model: str = None, debug: bool = False) -> Dict:\n",
    "    \"\"\"\n",
    "    Detects if input is relevant to VAANI and identifies transaction type.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        model_config = get_model_config()\n",
    "        model = model_config['model']\n",
    "    else:\n",
    "        model_config = get_model_config(model)\n",
    "    \n",
    "    prompt = get_prompt('intent', text_input=text_input)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        if 'gpt' in model:\n",
    "            client = get_openai_client()\n",
    "            if not client:\n",
    "                return {'error': 'OpenAI client not available'}\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.2,\n",
    "                max_tokens=200\n",
    "            )\n",
    "            raw_response = response.choices[0].message.content\n",
    "        \n",
    "        elif 'claude' in model:\n",
    "            client = get_anthropic_client()\n",
    "            if not client:\n",
    "                return {'error': 'Anthropic client not available'}\n",
    "            \n",
    "            response = client.messages.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.2,\n",
    "                max_tokens=200\n",
    "            )\n",
    "            raw_response = response.content[0].text\n",
    "        \n",
    "        else:\n",
    "            return {'error': f'Unknown model: {model}'}\n",
    "        \n",
    "        time_taken = time.time() - start_time\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"\\nðŸ” DEBUG - Intent Raw Response:\")\n",
    "            print(f\"{raw_response}\")\n",
    "        \n",
    "        if not raw_response or not raw_response.strip():\n",
    "            return {'error': 'Empty response from model', 'model_used': model, 'time_taken': round(time_taken, 2)}\n",
    "        \n",
    "        import json\n",
    "        clean_json = extract_json_from_response(raw_response)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"\\nðŸ” DEBUG - Cleaned JSON:\")\n",
    "            print(f\"{clean_json}\")\n",
    "        \n",
    "        intent_result = json.loads(clean_json)\n",
    "        intent_result['raw_response'] = raw_response\n",
    "        intent_result['model_used'] = model\n",
    "        intent_result['time_taken'] = round(time_taken, 2)\n",
    "        \n",
    "        return intent_result\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        return {\n",
    "            'error': f'JSON parsing failed: {str(e)}',\n",
    "            'raw_response': raw_response if 'raw_response' in locals() else 'No response',\n",
    "            'cleaned_json': clean_json if 'clean_json' in locals() else 'No JSON',\n",
    "            'model_used': model,\n",
    "            'time_taken': round(time.time() - start_time, 2)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'error': str(e),\n",
    "            'model_used': model,\n",
    "            'time_taken': round(time.time() - start_time, 2)\n",
    "        }\n",
    "\n",
    "print(\"âœ… Intent detection agent loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5174d6a-9ffc-4fd5-8b6c-48cddca7a4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Transaction extraction agent loaded\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 11: TRANSACTION-SPECIFIC EXTRACTION AGENT (UPDATED)\n",
    "Description: Extracts data using the appropriate transaction-type prompt\n",
    "\"\"\"\n",
    "\n",
    "def agent_transaction_extractor(text_input: str, transaction_type: str, model: str = None, debug: bool = False) -> Dict:\n",
    "    \"\"\"\n",
    "    Extracts transaction details using the appropriate prompt for the transaction type.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        model_config = get_model_config()\n",
    "        model = model_config['model']\n",
    "    else:\n",
    "        model_config = get_model_config(model)\n",
    "    \n",
    "    prompt = get_prompt(transaction_type, text_input=text_input)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        if 'gpt' in model:\n",
    "            client = get_openai_client()\n",
    "            if not client:\n",
    "                return {'error': 'OpenAI client not available'}\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=model_config.get('temperature', 0.3),\n",
    "                max_tokens=model_config.get('max_tokens', 500)\n",
    "            )\n",
    "            raw_response = response.choices[0].message.content\n",
    "        \n",
    "        elif 'claude' in model:\n",
    "            client = get_anthropic_client()\n",
    "            if not client:\n",
    "                return {'error': 'Anthropic client not available'}\n",
    "            \n",
    "            response = client.messages.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=model_config.get('temperature', 0.3),\n",
    "                max_tokens=model_config.get('max_tokens', 500)\n",
    "            )\n",
    "            raw_response = response.content[0].text\n",
    "        \n",
    "        else:\n",
    "            return {'error': f'Unknown model: {model}'}\n",
    "        \n",
    "        time_taken = time.time() - start_time\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"\\nðŸ” DEBUG - Raw Response:\")\n",
    "            print(f\"{raw_response}\")\n",
    "        \n",
    "        if not raw_response or not raw_response.strip():\n",
    "            return {\n",
    "                'error': 'Empty response from model',\n",
    "                'raw_response': raw_response,\n",
    "                'transaction_type': transaction_type,\n",
    "                'model_used': model,\n",
    "                'time_taken': round(time_taken, 2)\n",
    "            }\n",
    "        \n",
    "        import json\n",
    "        clean_json = extract_json_from_response(raw_response)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"\\nðŸ” DEBUG - Cleaned JSON:\")\n",
    "            print(f\"{clean_json}\")\n",
    "        \n",
    "        extracted = json.loads(clean_json)\n",
    "        extracted['transaction_type'] = transaction_type\n",
    "        extracted['raw_response'] = raw_response\n",
    "        extracted['model_used'] = model\n",
    "        extracted['time_taken'] = round(time_taken, 2)\n",
    "        \n",
    "        return extracted\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        return {\n",
    "            'error': f'JSON parsing failed: {str(e)}',\n",
    "            'raw_response': raw_response if 'raw_response' in locals() else 'No response',\n",
    "            'cleaned_json': clean_json if 'clean_json' in locals() else 'No JSON extracted',\n",
    "            'transaction_type': transaction_type,\n",
    "            'model_used': model,\n",
    "            'time_taken': round(time.time() - start_time, 2)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'error': f'Unexpected error: {str(e)}',\n",
    "            'raw_response': raw_response if 'raw_response' in locals() else 'No response',\n",
    "            'transaction_type': transaction_type,\n",
    "            'model_used': model,\n",
    "            'time_taken': round(time.time() - start_time, 2)\n",
    "        }\n",
    "\n",
    "print(\"âœ… Transaction extraction agent loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "978eb447-f17a-46c6-862f-8a2ba0a74bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Smart router with greeting support loaded\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 12: SMART ROUTER WITH INTENT DETECTION (UPDATED)\n",
    "Description: Complete flow - Intent detection â†’ Transaction extraction OR Greeting response\n",
    "\"\"\"\n",
    "\n",
    "def route_with_intent(text_input: str, transaction_type: str = None, model: str = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Smart router with intent detection and greeting handling.\n",
    "    \n",
    "    Flow:\n",
    "    1. If transaction_type provided â†’ Skip intent detection, go directly to extraction\n",
    "    2. If transaction_type NOT provided â†’ Run intent detection first\n",
    "    3. If greeting detected â†’ Return friendly response\n",
    "    4. If not_relevant â†’ Return not relevant message\n",
    "    5. Otherwise â†’ Extract transaction data\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'input': text_input,\n",
    "        'intent_detection_skipped': transaction_type is not None\n",
    "    }\n",
    "    \n",
    "    # Step 1: Intent Detection (if needed)\n",
    "    if transaction_type is None:\n",
    "        print(\"ðŸ” Running intent detection...\")\n",
    "        intent_result = agent_intent_detector(text_input, model)\n",
    "        \n",
    "        if 'error' in intent_result:\n",
    "            return {'error': f\"Intent detection failed: {intent_result['error']}\"}\n",
    "        \n",
    "        result['intent'] = intent_result\n",
    "        \n",
    "        # Check if relevant\n",
    "        if not intent_result.get('is_relevant'):\n",
    "            result['status'] = 'not_relevant'\n",
    "            result['message'] = \"Input is not relevant for business transactions\"\n",
    "            return result\n",
    "        \n",
    "        # Get transaction type from intent\n",
    "        transaction_type = intent_result.get('transaction_type')\n",
    "        \n",
    "        # Handle greetings\n",
    "        if transaction_type == 'greeting':\n",
    "            result['status'] = 'greeting'\n",
    "            result['message'] = \"\"\"Hello! I'm VAANI, your voice assistant for business management. ðŸ˜Š\n",
    "\n",
    "I can help you with:\n",
    "âœ… Recording expenses\n",
    "âœ… Creating sales & invoices  \n",
    "âœ… Managing purchases\n",
    "âœ… Tracking payments (in/out)\n",
    "\n",
    "Just tell me what you'd like to do! For example:\n",
    "\"Add expense chai 50 rupees\"\n",
    "\"Sharma ji bought 5kg rice for 250\"\n",
    "\"Received payment 2000 from Kumar\"\n",
    "\n",
    "How can I help you today?\"\"\"\n",
    "            return result\n",
    "        \n",
    "        # Handle not relevant\n",
    "        if transaction_type == 'not_relevant':\n",
    "            result['status'] = 'not_relevant'\n",
    "            result['message'] = intent_result.get('reason', \"I can only help with business transactions right now.\")\n",
    "            return result\n",
    "    \n",
    "    else:\n",
    "        print(f\"â­ï¸  Skipping intent detection, using: {transaction_type}\")\n",
    "        result['intent'] = {'transaction_type': transaction_type, 'skipped': True}\n",
    "    \n",
    "    # Step 2: Transaction-specific extraction\n",
    "    print(f\"ðŸ“Š Extracting {transaction_type} data...\")\n",
    "    extraction_result = agent_transaction_extractor(text_input, transaction_type, model)\n",
    "    \n",
    "    if 'error' in extraction_result:\n",
    "        result['error'] = f\"Extraction failed: {extraction_result['error']}\"\n",
    "        return result\n",
    "    \n",
    "    result['extraction'] = extraction_result\n",
    "    result['status'] = 'success'\n",
    "    result['transaction_type'] = transaction_type\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"âœ… Smart router with greeting support loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15fbaeaa-c26d-4597-9024-4207df347864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“š VAANI FUNCTIONS AVAILABLE\n",
      "=====================================================\n",
      "\n",
      "ðŸ¤– AI AGENTS:\n",
      "1. agent_intent_detector(text_input, model=None, debug=False)\n",
      "   - Detects transaction type\n",
      "   \n",
      "2. agent_transaction_extractor(text_input, transaction_type, model=None, debug=False)\n",
      "   - Extracts transaction data\n",
      "   \n",
      "3. route_with_intent(text_input, transaction_type=None, model=None)\n",
      "   - Smart router (RECOMMENDED)\n",
      "\n",
      "ðŸ› ï¸ UTILITIES:\n",
      "- format_amount(amount) - Clean amount formatting\n",
      "- validate_extraction(result) - Validate extraction results\n",
      "- extract_json_from_response(text) - Clean JSON from LLM\n",
      "- CellTimer() - Track execution time\n",
      "\n",
      "ðŸ”Œ CLIENTS:\n",
      "- get_openai_client() - Get OpenAI client\n",
      "- get_anthropic_client() - Get Anthropic client\n",
      "\n",
      "ðŸ“‹ HELPERS:\n",
      "- view_prompt(prompt_type) - View a prompt\n",
      "- view_all_prompts() - List all prompts\n",
      "- view_transaction_fields() - Show field requirements\n",
      "\n",
      "=====================================================\n",
      "âœ… All functions loaded and ready!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 16: SUMMARY\n",
    "Description: Shows all available functions\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ“š VAANI FUNCTIONS AVAILABLE\n",
    "=====================================================\n",
    "\n",
    "ðŸ¤– AI AGENTS:\n",
    "1. agent_intent_detector(text_input, model=None, debug=False)\n",
    "   - Detects transaction type\n",
    "   \n",
    "2. agent_transaction_extractor(text_input, transaction_type, model=None, debug=False)\n",
    "   - Extracts transaction data\n",
    "   \n",
    "3. route_with_intent(text_input, transaction_type=None, model=None)\n",
    "   - Smart router (RECOMMENDED)\n",
    "\n",
    "ðŸ› ï¸ UTILITIES:\n",
    "- format_amount(amount) - Clean amount formatting\n",
    "- validate_extraction(result) - Validate extraction results\n",
    "- extract_json_from_response(text) - Clean JSON from LLM\n",
    "- CellTimer() - Track execution time\n",
    "\n",
    "ðŸ”Œ CLIENTS:\n",
    "- get_openai_client() - Get OpenAI client\n",
    "- get_anthropic_client() - Get Anthropic client\n",
    "\n",
    "ðŸ“‹ HELPERS:\n",
    "- view_prompt(prompt_type) - View a prompt\n",
    "- view_all_prompts() - List all prompts\n",
    "- view_transaction_fields() - Show field requirements\n",
    "\n",
    "=====================================================\n",
    "âœ… All functions loaded and ready!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aca8e58d-20d8-48e1-8e31-1f815dc8ef88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Gemini functions loaded!\n",
      "\n",
      "Available:\n",
      "- get_gemini_model(model_name)\n",
      "- call_gemini(prompt, model_name, return_json, debug)\n",
      "- gemini_intent_detector(text_input, model_name, debug)\n",
      "- gemini_expense_extractor(text_input, model_name, debug)\n",
      "- gemini_with_fallback(prompt, return_json)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL: GEMINI MODEL FUNCTIONS\n",
    "\"\"\"\n",
    "\n",
    "def get_gemini_model(model_name='gemini_2_flash'):\n",
    "    \"\"\"\n",
    "    Get configured Gemini model from registry\n",
    "    \n",
    "    Args:\n",
    "        model_name: 'gemini_2_flash', 'gemini_15_flash', or 'gemini_15_pro'\n",
    "    \n",
    "    Returns:\n",
    "        Configured GenerativeModel instance\n",
    "    \"\"\"\n",
    "    if model_name not in GEMINI_MODELS:\n",
    "        print(f\"âš ï¸ Model {model_name} not found, using gemini_2_flash\")\n",
    "        model_name = 'gemini_2_flash'\n",
    "    \n",
    "    config = GEMINI_MODELS[model_name]\n",
    "    \n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=config['model'],\n",
    "        generation_config={\n",
    "            'temperature': config['temperature'],\n",
    "            'max_output_tokens': config['max_tokens']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def call_gemini(prompt, model_name='gemini_2_flash', return_json=True, debug=False):\n",
    "    \"\"\"\n",
    "    Call Gemini API with prompt\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt text\n",
    "        model_name: Which Gemini model to use\n",
    "        return_json: Try to parse response as JSON\n",
    "        debug: Print debug info\n",
    "    \n",
    "    Returns:\n",
    "        Parsed JSON or raw text response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = get_gemini_model(model_name)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"ðŸ” Using: {GEMINI_MODELS[model_name]['model']}\")\n",
    "            print(f\"ðŸ“ Prompt length: {len(prompt)} chars\")\n",
    "        \n",
    "        response = model.generate_content(prompt)\n",
    "        result = response.text\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"âœ… Response length: {len(result)} chars\")\n",
    "        \n",
    "        if return_json:\n",
    "            # Clean JSON from response\n",
    "            if '```json' in result:\n",
    "                result = result.split('```json')[1].split('```')[0].strip()\n",
    "            elif '```' in result:\n",
    "                result = result.split('```')[1].split('```')[0].strip()\n",
    "            \n",
    "            try:\n",
    "                return json.loads(result)\n",
    "            except:\n",
    "                if debug:\n",
    "                    print(\"âš ï¸ Could not parse JSON, returning raw text\")\n",
    "                return result\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def gemini_intent_detector(text_input, model_name='gemini_2_flash', debug=False):\n",
    "    \"\"\"\n",
    "    Detect transaction intent using Gemini\n",
    "    \n",
    "    Args:\n",
    "        text_input: Voice input text\n",
    "        model_name: Which model to use\n",
    "        debug: Show debug info\n",
    "    \n",
    "    Returns:\n",
    "        {\"intent\": \"expense|sale|payment_in|payment_out|purchase|query|other\"}\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Analyze this voice input and detect the intent.\n",
    "\n",
    "Voice Input: \"{text_input}\"\n",
    "\n",
    "Return ONLY a JSON object:\n",
    "{{\n",
    "    \"intent\": \"expense or sale or payment_in or payment_out or purchase or query or other\"\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "- expense: adding/recording expense\n",
    "- sale: creating sale/invoice\n",
    "- payment_in: receiving payment\n",
    "- payment_out: making payment  \n",
    "- purchase: buying/purchasing\n",
    "- query: asking question\n",
    "- other: unclear/unrelated\"\"\"\n",
    "    \n",
    "    result = call_gemini(prompt, model_name, return_json=True, debug=debug)\n",
    "    \n",
    "    if result and 'intent' in result:\n",
    "        return result\n",
    "    \n",
    "    return {\"intent\": \"other\"}\n",
    "\n",
    "\n",
    "def gemini_expense_extractor(text_input, model_name='gemini_2_flash', debug=False):\n",
    "    \"\"\"\n",
    "    Extract expense data using Gemini\n",
    "    \n",
    "    Args:\n",
    "        text_input: Voice input text\n",
    "        model_name: Which model to use\n",
    "        debug: Show debug info\n",
    "    \n",
    "    Returns:\n",
    "        Expense data JSON\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Extract expense details from this voice input.\n",
    "\n",
    "Voice Input: \"{text_input}\"\n",
    "\n",
    "Return ONLY a JSON object:\n",
    "{{\n",
    "    \"items\": [\n",
    "        {{\n",
    "            \"item_name\": \"name or empty string\",\n",
    "            \"amount\": number or null\n",
    "        }}\n",
    "    ],\n",
    "    \"date\": \"YYYY-MM-DD or null\",\n",
    "    \"payment_type\": \"Cash or Online or null\",\n",
    "    \"category\": \"suggested category or null\"\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "- Extract ALL items (max 10)\n",
    "- Amount as number only\n",
    "- If date not mentioned, use null\n",
    "- If payment not mentioned, use null\"\"\"\n",
    "    \n",
    "    result = call_gemini(prompt, model_name, return_json=True, debug=debug)\n",
    "    return result\n",
    "\n",
    "\n",
    "def gemini_with_fallback(prompt, return_json=True):\n",
    "    \"\"\"\n",
    "    Try all Gemini models with fallback\n",
    "    Order: 2.0 Flash -> 1.5 Flash -> 1.5 Pro\n",
    "    \"\"\"\n",
    "    models = ['gemini_2_flash', 'gemini_15_flash', 'gemini_15_pro']\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"ðŸ”„ Trying {model}...\")\n",
    "        result = call_gemini(prompt, model, return_json)\n",
    "        \n",
    "        if result:\n",
    "            print(f\"âœ… Success with {model}\")\n",
    "            return result\n",
    "    \n",
    "    print(\"âŒ All models failed\")\n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"âœ… Gemini functions loaded!\")\n",
    "print(\"\\nAvailable:\")\n",
    "print(\"- get_gemini_model(model_name)\")\n",
    "print(\"- call_gemini(prompt, model_name, return_json, debug)\")\n",
    "print(\"- gemini_intent_detector(text_input, model_name, debug)\")\n",
    "print(\"- gemini_expense_extractor(text_input, model_name, debug)\")\n",
    "print(\"- gemini_with_fallback(prompt, return_json)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39a1325-9b22-41b3-a607-aea779e5761f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c39e0-a4d5-4380-95ed-ce981fbe81e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dbca7e-3f4e-44e6-82a6-acc88cd02d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc03374-c667-4c53-9e6c-192a925ccbec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c589095f-5891-49d1-8ad3-abb24dcc9b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a549dfd-c09c-4081-8113-93aaea07bbcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
